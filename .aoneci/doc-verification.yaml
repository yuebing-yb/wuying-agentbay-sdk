name: "Doc Examples Verification"

params:
  pattern:
    name: Filter Pattern
    type: string
    default: ""
    description: "Only run verification on files matching this pattern"
  limit:
    name: Limit Examples
    type: string
    default: ""
    description: "Limit the number of examples to verify (for testing)"

traits:
  - type: notification
    properties:
      types: dingtalk
      when: all
      users: 187788
      content: |
        ---
        - **Doc Verification Report**
            - @${{git.repo}}
            - **Branch**: ${{git.branch}}
            - **Commit**: ${{git.commitId}}
            - **Changed By**: ${{git.employeeId}}
                      
        ðŸ“š **æ–‡æ¡£ç¤ºä¾‹ä»£ç éªŒè¯æŠ¥å‘Š**
        
        ðŸ“Š **éªŒè¯ç»“æžœï¼š** ${{jobs.doc-verification.outputs.verification_summary}}
        
        **å¤±è´¥è¯¦æƒ…ï¼š**
        ${{jobs.doc-verification.outputs.failed_items}}
        
        ðŸ’¡ è¯·æŸ¥çœ‹CIè¯¦æƒ…é¡µé¢èŽ·å–è¯¦ç»†åˆ†æžã€‚

jobs:
  doc-verification:
    name: "Doc Examples Verification"
    runs-on:
      - "4-16Gi"
    image: alios-8u
    timeout: 5h
    retry:
      max: 1
      when:
        - runner_system_failure
    outputs:
      verification_summary: ${{steps.run-verification.outputs.verification_summary}}
      failed_items: ${{steps.run-verification.outputs.failed_items}}
    steps:
      - uses: checkout
      
      - uses: setup-env
        inputs:
          python-version: '3.12'
          node-version: '18'
          npm-version: '10'
          npm-cache: true
          
      - id: run-verification
        name: "Run Verification"
        run: |
          echo "ðŸ”„ Installing dependencies..."
          
          # Setup Python environment
          python -m pip config set global.timeout 600
          python -m pip config set global.retries 5
          python -m pip install --upgrade pip
          
          # Install project dependencies
          echo "ðŸ Installing project dependencies..."
          if [ -d "python" ]; then
            pip install -e ./python/
          else
            echo "âŒ Python directory not found"
            ls -la
            exit 1
          fi
          
          # Install test and AI dependencies
          pip install "darabonba-core>=1.0.0,<2.0.0" \
                      "alibabacloud-tea-openapi>=0.4.0rc3" \
                      "cryptography>=44.0.0" \
                      "httpx>=0.28.1" \
                      "alibabacloud-tea-util>=0.3.13" \
                      "python-dotenv>=1.0.0" \
                      "pydantic>=2.0" \
                      "playwright>=1.5.0" \
                      "loguru>=0.7.0" \
                      pytest pytest-asyncio langchain-openai langgraph langchain-core markdown-it-py
          
          # Install playwright browsers and system dependencies
          echo "ðŸŽ­ Installing Playwright browsers and system dependencies..."
          playwright install-deps || echo "âš ï¸ Playwright system deps install failed"
          playwright install chromium || echo "âš ï¸ Playwright install failed"
          
          echo "ðŸ”§ Setting up environment..."
          cd /aoneci/runner/work/source || exit 1
          
          export PYTHONPATH="${PYTHONPATH}:$(pwd)/python"
          export AGENTBAY_API_KEY="${{secrets.AGENTBAY_API_KEY}}"
          export DASHSCOPE_API_KEY="${{secrets.DASHSCOPE_API_KEY}}"
          
          echo "ðŸš€ Starting Doc Verification (Batch Mode)..."
          
          # Build command arguments - ä½¿ç”¨æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬
          CMD="python scripts/verify_doc_examples_batch.py --report test_report.md"
          if [ -n "${{params.pattern}}" ]; then
            CMD="$CMD --pattern ${{params.pattern}}"
          fi
          if [ -n "${{params.limit}}" ]; then
            CMD="$CMD --limit ${{params.limit}}"
          fi
          
          echo "Executing: $CMD"
          
          # Run verification
          if $CMD; then
            echo "âœ… Verification script completed successfully"
          else
            echo "âŒ Verification script failed"
            # Don't exit immediately, we want to process the report
          fi
          
          # Process report
          if [ -f "test_report.md" ]; then
            echo "âœ… Report generated"
            
            # Extract statistics for summary
            TOTAL=$(grep -o "\*\*æ€»è®¡æ–‡ä»¶\*\*: [0-9]\+" test_report.md | grep -o "[0-9]\+" || echo "0")
            PASSED=$(grep -o "\*\*âœ… é€šè¿‡\*\*: [0-9]\+" test_report.md | grep -o "[0-9]\+" || echo "0")
            FAILED=$(grep -o "\*\*âŒ å¤±è´¥\*\*: [0-9]\+" test_report.md | grep -o "[0-9]\+" || echo "0")
            
            SUMMARY="$TOTAL Files | âœ… $PASSED Passed | âŒ $FAILED Failed"
            
            # Generate failed list for notification
            FAILED_LIST=""
            if [ "$FAILED" -gt 0 ]; then
              # Extract failed files from the markdown table (skip header and separator)
              grep "^| \`" test_report.md | head -5 | while IFS= read -r line; do
                 # Extract file path from table cell `path`
                 file=$(echo "$line" | cut -d'|' -f2 | tr -d '`' | xargs)
                 error=$(echo "$line" | cut -d'|' -f3 | tr -d '*' | xargs)
                 echo "- $file ($error)" >> /tmp/failed_list
              done
              
              if [ -f /tmp/failed_list ]; then
                FAILED_LIST=$(cat /tmp/failed_list)
                if [ "$FAILED" -gt 5 ]; then
                   FAILED_LIST="$FAILED_LIST\n... (æ›´å¤šè¯·æŸ¥çœ‹CIæŠ¥å‘Š)"
                fi
                rm -f /tmp/failed_list
              fi
            else
              FAILED_LIST="âœ… æ–‡æ¡£ç¤ºä¾‹éªŒè¯é€šè¿‡ï¼"
            fi
            
            # Output to step outputs
            echo "$SUMMARY" > "${{outputs.verification_summary.path}}"
            printf "%s\n" "$FAILED_LIST" > "${{outputs.failed_items.path}}"
            
            # Add report to CI summary
            echo "## ðŸ“š Doc Verification Report" >> $AONE_CI_SUMMARY_MD
            cat test_report.md >> $AONE_CI_SUMMARY_MD
            
            # Archive report
            mkdir -p artifacts
            cp test_report.md artifacts/doc_verification_report.md
            
            # Fail pipeline if doc issues found
            if [ "$FAILED" -gt 0 ]; then
              echo "âŒ Found documentation issues"
              exit 1
            fi
          else
            echo "âš ï¸ No report generated"
            echo "Failed to generate verification report" > "${{outputs.verification_summary.path}}"
            exit 1
          fi

