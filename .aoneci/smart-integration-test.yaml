name: "Smart Integration Test"

triggers:
  schedule:
    - cron: '0 3 * * 1-5'  # å·¥ä½œæ—¥ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰å‡Œæ™¨3ç‚¹è§¦å‘
      always: false        # ç›¸åŒcommitIdåªæ‰§è¡Œä¸€æ¬¡

params:
  test_type:
    name: Test Type
    type: string
    default: "all"
    options:
      - "all"
      - "python"
      - "typescript"
      - "golang"

traits:
  - type: notification
    properties:
      types: dingtalk
      when: all
      users: 187788
      content: |
        ---
        - **Test Report**
            - @${{git.repo}}
            - **Branch**: ${{git.branch}}
            - **Commit**: ${{git.commitId}}
            - **Changed By**: ${{git.employeeId}}
                      
        ğŸ¤– **æ™ºèƒ½é›†æˆæµ‹è¯•æŠ¥å‘Š**
        
        ğŸ“Š **æµ‹è¯•ç»“æœï¼š** ${{jobs.smart-integration-test.outputs.test_summary}}
        
        **å¤±è´¥æµ‹è¯•è¯¦æƒ…ï¼š**
        ${{jobs.smart-integration-test.outputs.failed_tests}}
        
        ğŸ’¡ è¯·æŸ¥çœ‹CIè¯¦æƒ…é¡µé¢è·å–AIåˆ†æå’Œä¿®å¤å»ºè®®ã€‚

jobs:
  smart-integration-test:
    name: "Smart Integration Test"
    runs-on:
      - "4-16Gi"
    image: alios-8u
    timeout: 5h
    retry:
      max: 2
      when:
        - runner_system_failure
        - stuck_or_timeout_failure
    outputs:
      test_summary: ${{steps.run-smart-tests.outputs.test_summary}}
      failed_tests: ${{steps.run-smart-tests.outputs.failed_tests}}
    steps:
      - uses: checkout
      
      - uses: setup-env
        inputs:
          python-version: '3.12'
          node-version: '18'
          npm-version: '10'
          npm-cache: true
          go-version: '1.24.3'
          go-mod-cache: true
          go-cache: true
          
      - id: run-smart-tests
        name: "Run Smart Tests"
        run: |
          echo "ğŸ”„ Installing dependencies for Smart Test Runner..."
          
          # Setup Golang dependencies if needed
          if [ "${{params.test_type}}" = "golang" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "ğŸ¹ Setting up Golang environment..."
            # Configure Go environment (setup-env already installed Go)
            go env -w GOPROXY=https://goproxy.cn,direct
            go env -w GOSUMDB=sum.golang.google.cn
            go env -w GOTOOLCHAIN=local
            go version
            
            if [ -d "golang" ]; then
              cd golang
              go mod download
              # Install Playwright browsers for Golang tests
              echo "ğŸ­ Installing Playwright browsers for Golang..."
              go run github.com/playwright-community/playwright-go/cmd/playwright@latest install --with-deps
              cd ..
            fi
          fi
          
          # Setup TypeScript dependencies if needed  
          if [ "${{params.test_type}}" = "typescript" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "ğŸ“œ Setting up TypeScript environment..."
            # Node.js and npm already installed by setup-env
            node --version
            npm --version
            
            if [ -d "typescript" ]; then
              cd typescript
              # Use npm ci like the working unit tests
              npm config set fetch-timeout 300000
              npm config set fetch-retry-mintimeout 20000
              npm config set fetch-retry-maxtimeout 120000
              npm ci --prefer-offline --no-audit
              
              # Verify ts-jest installation
              echo "ğŸ” Verifying ts-jest installation..."
              npm list ts-jest || echo "âš ï¸ ts-jest not found"
              
              # Install Playwright browsers for TypeScript tests
              echo "ğŸ­ Installing Playwright browsers for TypeScript..."
              npx playwright install --with-deps || echo "âš ï¸ Playwright install failed"
              cd ..
            fi
          fi
          
          # Setup Python environment (always needed for the test runner itself)
          python -m pip config set global.timeout 600
          python -m pip config set global.retries 5
          python -m pip install --upgrade pip
          
          # Install Python dependencies similar to unit test job
          pip install "darabonba-core>=1.0.0,<2.0.0" \
                      "alibabacloud-tea-openapi>=0.4.0rc3" \
                      "cryptography>=44.0.0" \
                      "httpx>=0.28.1" \
                      "alibabacloud-tea-util>=0.3.13" \
                      "python-dotenv>=1.0.0" \
                      "pydantic>=2.0" \
                      "playwright>=1.5.0" \
                      "loguru>=0.7.0"
          
          # Install test dependencies
          pip install pytest pytest-cov pytest-asyncio
          
          # Install AI dependencies
          pip install langchain-openai langgraph langchain-core
          
          echo "ğŸ”§ Setting up environment..."
          # Ensure we're in the correct working directory
          cd /aoneci/runner/work/source || exit 1
          echo "ğŸ“‚ Current working directory: $(pwd)"
          
          # Install SDK in editable mode with correct path
          if [ -d "python" ]; then
            pip install -e ./python/
            echo "âœ… Python SDK installed successfully"
          else
            echo "âŒ Python directory not found"
            ls -la
            exit 1
          fi
          
          # Set PYTHONPATH similar to unit test job
          export PYTHONPATH="${PYTHONPATH}:$(pwd)/python"
          export AGENTBAY_API_KEY="${{secrets.AGENTBAY_API_KEY}}"
          export DASHSCOPE_API_KEY="${{secrets.DASHSCOPE_API_KEY}}"
          
          echo "ğŸš€ Starting Smart Test Runner..."
          echo "Working directory: $(pwd)"
          echo "Python path: $PYTHONPATH"
          
          # Set up environment for test execution
          # Ensure we have the correct working directory
          cd /aoneci/runner/work/source || exit 1
          
          # Set Go environment variables if needed
          if [ "${{params.test_type}}" = "golang" ] || [ "${{params.test_type}}" = "all" ]; then
            export GOPROXY=https://goproxy.cn,direct
            export GOSUMDB=sum.golang.google.cn  
            export GOTOOLCHAIN=local
            echo "ğŸ” Checking Go installation..."
            which go || echo "âŒ Go not found in PATH"
            go version || echo "âŒ Go version check failed"
          fi
          
          if [ "${{params.test_type}}" = "typescript" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "ğŸ” Checking Node.js installation..."
            which node || echo "âŒ Node not found in PATH"
            node --version || echo "âŒ Node version check failed"
            which npm || echo "âŒ npm not found in PATH"
            npm --version || echo "âŒ npm version check failed"
          fi
          
          # Run the smart test runner
          if python scripts/smart_test_runner.py --test-type="${{params.test_type}}"; then
            echo "âœ… Smart Test Runner completed successfully"
            
            # Check if test report was generated
          if [ -f "test_report.md" ]; then
            echo "âœ… Test report generated successfully"
            
            # Display in CI Summary using è±†è…å— format
            # Extract test statistics
            TOTAL_TESTS=$(grep -o "[0-9]\+ Tests" test_report.md | grep -o "[0-9]\+" || echo "0")
            PASSED_TESTS=$(grep -o "âœ… [0-9]\+ Passed" test_report.md | grep -o "[0-9]\+" || echo "0")
            FAILED_TESTS=$(grep -o "âŒ [0-9]\+ Failed" test_report.md | grep -o "[0-9]\+" || echo "0")
            
            # Generate TEST_CASE è±†è…å— - ensure all required fields are present
            SKIPPED_TESTS=0  # Default to 0 if no skipped tests
            
            # Debug: Print variables before generating JSON
            echo "Debug: TOTAL_TESTS=$TOTAL_TESTS, PASSED_TESTS=$PASSED_TESTS, FAILED_TESTS=$FAILED_TESTS, SKIPPED_TESTS=$SKIPPED_TESTS"
            
            # Ensure variables are not empty
            FAILED_TESTS=${FAILED_TESTS:-0}
            PASSED_TESTS=${PASSED_TESTS:-0}
            SKIPPED_TESTS=${SKIPPED_TESTS:-0}
            
            # Generate JSON with explicit field values - ensure all three required fields are present
            TEST_CASE_JSON="{\"name\": \"æ™ºèƒ½é›†æˆæµ‹è¯•\", \"failed\": $FAILED_TESTS, \"skipped\": $SKIPPED_TESTS, \"passed\": $PASSED_TESTS}"
            echo "Debug: Generated TEST_CASE_JSON=$TEST_CASE_JSON"
            echo "TEST_CASE=$TEST_CASE_JSON" >> $AONE_CI_SUMMARY
            
            # Generate test summary for notification
            TEST_CASE_SUMMARY="$TOTAL_TESTS Tests | âœ… $PASSED_TESTS Passed | âŒ $FAILED_TESTS Failed"
            
            # Extract failed test IDs for notification with language info
            if [ "$FAILED_TESTS" -gt 0 ]; then
              # Create proper table format with complete rows
              {
                echo "| è¯­è¨€ | æµ‹è¯•ç”¨ä¾‹ |"
                echo "| :-: | :-: |"
              } > /tmp/failed_tests_table
              
              # Extract failed tests with language detection
              grep -A1 "âŒ å¤±è´¥æµ‹è¯•" test_report.md | grep "æµ‹è¯•ID:" | head -10 | while IFS= read -r line; do
                test_id=$(echo "$line" | sed 's/æµ‹è¯•ID: //')
                
                # Detect language based on test ID patterns - fix the logic
                if echo "$test_id" | grep -q "golang\|\.go\|github.com.*golang"; then
                  lang="Golang"
                elif echo "$test_id" | grep -q "python\|\.py\|pytest"; then
                  lang="Python"
                elif echo "$test_id" | grep -q "typescript\|\.ts\|\.js\|npm\|jest"; then
                  lang="TypeScript"
                else
                  lang="æœªçŸ¥"
                fi
                
                # Clean up test ID - remove common prefixes and paths
                clean_test_id=$(echo "$test_id" | sed -e 's/^.*golang\/tests\/pkg\/integration\.//' -e 's/^.*\///' -e 's/^test_//' -e 's/^integration_//')
                
                echo "| $lang | $clean_test_id |" >> /tmp/failed_tests_table
              done
              
              if [ -f /tmp/failed_tests_table ]; then
                FAILED_TEST_LIST=$(cat /tmp/failed_tests_table | tr '\n' ' ')
                rm -f /tmp/failed_tests_table
              fi
            else
              FAILED_TEST_LIST="âœ… æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼"
            fi
            
            # Output to step outputs for notification access
            echo "$TEST_CASE_SUMMARY" > "${{outputs.test_summary.path}}"
            printf "%s\n" "$FAILED_TEST_LIST" > "${{outputs.failed_tests.path}}"
            
            
            
            # If there are failures, also add detailed results to Markdown summary
            if [ "$FAILED_TESTS" -gt 0 ]; then
              # Also show detailed results in markdown format for failed tests
              echo "## ğŸ¤– Smart Integration Test Report" >> $AONE_CI_SUMMARY_MD
              echo "" >> $AONE_CI_SUMMARY_MD
              echo "ğŸ“Š **Summary**: $TOTAL_TESTS Tests | âœ… $PASSED_TESTS Passed | âŒ $FAILED_TESTS Failed" >> $AONE_CI_SUMMARY_MD
              echo "" >> $AONE_CI_SUMMARY_MD
              echo "### ğŸ“‹ å¤±è´¥æµ‹è¯•è¯¦æƒ…" >> $AONE_CI_SUMMARY_MD
              echo "" >> $AONE_CI_SUMMARY_MD
              
              # Process each failed test and create collapsible sections (exclude AI fix prompts)
              awk '
              BEGIN { 
                test_count = 0;
                in_test = 0;
                test_id = "";
                ai_analysis = "";
                output_snippet = "";
                current_section = "";
              }
              
              # Start of a new failed test
              /^âŒ å¤±è´¥æµ‹è¯•$/ {
                # Output previous test if exists
                if (test_count > 0 && test_id != "") {
                  print "<details>";
                  print "<summary>" test_count ". " test_id "</summary>";
                  print "";
                  
                  # AI Analysis subsection
                  if (ai_analysis != "") {
                    print "<details>";
                    print "<summary>ğŸ¤– AIåˆ†æ</summary>";
                    print "";
                    print ai_analysis;
                    print "</details>";
                    print "";
                  }
                  
                  # Output snippet subsection
                  if (output_snippet != "") {
                    print "<details>";
                    print "<summary>ğŸ“„ æ—¥å¿—ç‰‡æ®µ</summary>";
                    print "";
                    print "```";
                    print output_snippet;
                    print "```";
                    print "</details>";
                    print "";
                  }
                  
                  # Note about AI fix prompt
                  print "<details>";
                  print "<summary>ğŸ› ï¸ AIä¿®å¤æç¤ºè¯</summary>";
                  print "";
                  print "è¯·åœ¨CIæ—¥å¿—çš„"Display Test Results"æ­¥éª¤ä¸­æŸ¥çœ‹å®Œæ•´çš„AIä¿®å¤æç¤ºè¯ã€‚";
                  print "</details>";
                  print "";
                  
                  print "</details>";
                  print "";
                }
                
                # Reset for new test
                test_count++;
                in_test = 1;
                test_id = "";
                ai_analysis = "";
                output_snippet = "";
                current_section = "";
                next;
              }
              
              # Capture test ID
              in_test && /^æµ‹è¯•ID:/ {
                gsub(/^æµ‹è¯•ID: /, "", $0);
                test_id = $0;
                next;
              }
              
              # Section headers
              in_test && /^ğŸ¤– AI Analysis/ {
                current_section = "ai_analysis";
                next;
              }
              
              in_test && /^ğŸ“„ Output \(Snippet\)/ {
                current_section = "output_snippet";
                next;
              }
              
              in_test && /^ğŸ› ï¸ AIä¿®å¤æç¤ºè¯/ {
                current_section = "skip";  # Skip AI fix prompt content
                next;
              }
              
              # Skip code block markers
              in_test && /^```/ {
                next;
              }
              
              # End of test section
              /^---$/ {
                in_test = 0;
                current_section = "";
                next;
              }
              
              # Collect content based on current section
              in_test && current_section == "ai_analysis" && $0 != "" {
                ai_analysis = ai_analysis $0 "\n";
              }
              
              in_test && current_section == "output_snippet" && $0 != "" {
                output_snippet = output_snippet $0 "\n";
              }
              
              END {
                # Output the last test
                if (test_count > 0 && test_id != "") {
                  print "<details>";
                  print "<summary>" test_count ". " test_id "</summary>";
                  print "";
                  
                  # AI Analysis subsection
                  if (ai_analysis != "") {
                    print "<details>";
                    print "<summary>ğŸ¤– AIåˆ†æ</summary>";
                    print "";
                    print ai_analysis;
                    print "</details>";
                    print "";
                  }
                  
                  # Output snippet subsection
                  if (output_snippet != "") {
                    print "<details>";
                    print "<summary>ğŸ“„ æ—¥å¿—ç‰‡æ®µ</summary>";
                    print "";
                    print "```";
                    print output_snippet;
                    print "```";
                    print "</details>";
                    print "";
                  }
                  
                  # Note about AI fix prompt
                  print "<details>";
                  print "<summary>ğŸ› ï¸ AIä¿®å¤æç¤ºè¯</summary>";
                  print "";
                  print "è¯·åœ¨CIæ—¥å¿—çš„"Display Test Results"æ­¥éª¤ä¸­æŸ¥çœ‹å®Œæ•´çš„AIä¿®å¤æç¤ºè¯ã€‚";
                  print "</details>";
                  print "";
                  
                  print "</details>";
                  print "";
                }
                
                # Debug info
                print "<!-- Debug: Processed " test_count " failed tests -->";
              }
              ' test_report.md >> $AONE_CI_SUMMARY_MD
            fi
            
            # Archive as build artifacts
            mkdir -p artifacts
            cp test_report.md artifacts/smart_test_report.md
          else
            echo "âš ï¸ No test report generated"
            echo "## âŒ Smart Integration Test Report" >> $AONE_CI_SUMMARY_MD
            echo "No test report was generated. Check the logs above for errors." >> $AONE_CI_SUMMARY_MD
          fi
          else
            echo "âŒ Smart Test Runner failed with exit code $?"
            echo "âŒ CI pipeline will now fail"
            exit 1
          fi
          
      - id: display-test-results
        name: "Display Test Results"
        run: |
          if [ -f "test_report.md" ]; then
            # Show success summary from the summary line
            if grep -q "Tests |" test_report.md; then
              SUMMARY_LINE=$(grep "Tests |" test_report.md | head -1)
              echo "ğŸ“Š $SUMMARY_LINE"
              echo ""
            fi
            
            # Create individual collapsible sections for each failed test - only show AI fix prompt
            if grep -q "âŒ å¤±è´¥æµ‹è¯•" test_report.md; then
              echo "ğŸ“‹ å¤±è´¥æµ‹è¯•æ¦‚è§ˆï¼š"
              echo ""
              
              # Process each failed test and only show AI fix prompt in collapsible sections
              awk '
              BEGIN { 
                test_count = 0;
                in_test = 0;
                test_id = "";
                fix_prompt = "";
                current_section = "";
              }
              
              /^âŒ å¤±è´¥æµ‹è¯•/ {
                # If we have a previous test, output it
                if (test_count > 0 && test_id != "" && fix_prompt != "") {
                  print "::group::" test_count ". " test_id " - ğŸ› ï¸ AIä¿®å¤æç¤ºè¯";
                  print "```";
                  print fix_prompt;
                  print "```";
                  print "::endgroup::";
                  print "";
                }
                
                # Start new test
                test_count++;
                in_test = 1;
                test_id = "";
                fix_prompt = "";
                current_section = "";
                
                # Read next line which should contain the test ID
                if (getline > 0 && /^æµ‹è¯•ID:/) {
                  # Extract test ID from "æµ‹è¯•ID: test_id_here"
                  gsub(/^æµ‹è¯•ID: /, "", $0);
                  test_id = $0;
                }
                next;
              }
              
              /^ğŸ› ï¸ AIä¿®å¤æç¤ºè¯/ && in_test {
                current_section = "fix_prompt";
                next;
              }
              
              /^```/ && in_test {
                # Skip code block markers
                next;
              }
              
              /^---$/ && in_test {
                # End of current test section
                in_test = 0;
                current_section = "";
                next;
              }
              
              in_test && current_section == "fix_prompt" {
                fix_prompt = fix_prompt $0 "\n";
              }
              
              END {
                # Output the last test if exists
                if (test_count > 0 && test_id != "" && fix_prompt != "") {
                  print "::group::" test_count ". " test_id " - ğŸ› ï¸ AIä¿®å¤æç¤ºè¯";
                  print "```";
                  print fix_prompt;
                  print "```";
                  print "::endgroup::";
                }
              }
              ' test_report.md
              
            else
              echo "âœ… æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼"
            fi
          else
            echo "âš ï¸ æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨"
          fi

