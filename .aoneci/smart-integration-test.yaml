name: "Smart Integration Test"

params:
  test_type:
    name: Test Type
    type: string
    default: "all"
    options:
      - "all"
      - "python"
      - "typescript"
      - "golang"

jobs:
  smart-integration-test:
    name: "Smart Integration Test"
    runs-on:
      - "16-64Gi"
    image: alios-8u
    timeout: 3h
    retry:
      max: 2
      when:
        - runner_system_failure
        - stuck_or_timeout_failure
    steps:
      - uses: checkout
      
      - uses: setup-env
        inputs:
          python-version: '3.12'
          node-version: '18'
          npm-version: '10'
          npm-cache: true
          go-version: '1.24.3'
          go-mod-cache: true
          go-cache: true
          
      - id: run-smart-tests
        name: "Run Smart Tests"
        run: |
          echo "üîÑ Installing dependencies for Smart Test Runner..."
          
          # Setup Golang dependencies if needed
          if [ "${{params.test_type}}" = "golang" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "üêπ Setting up Golang environment..."
            # Configure Go environment (setup-env already installed Go)
            go env -w GOPROXY=https://goproxy.cn,direct
            go env -w GOSUMDB=sum.golang.google.cn
            go env -w GOTOOLCHAIN=local
            go version
            
            if [ -d "golang" ]; then
              cd golang
              go mod download
              # Install Playwright browsers for Golang tests
              echo "üé≠ Installing Playwright browsers for Golang..."
              go run github.com/playwright-community/playwright-go/cmd/playwright@latest install --with-deps
              cd ..
            fi
          fi
          
          # Setup TypeScript dependencies if needed  
          if [ "${{params.test_type}}" = "typescript" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "üìú Setting up TypeScript environment..."
            # Node.js and npm already installed by setup-env
            node --version
            npm --version
            
            if [ -d "typescript" ]; then
              cd typescript
              # Use npm ci like the working unit tests
              npm config set fetch-timeout 300000
              npm config set fetch-retry-mintimeout 20000
              npm config set fetch-retry-maxtimeout 120000
              npm ci --prefer-offline --no-audit
              
              # Verify ts-jest installation
              echo "üîç Verifying ts-jest installation..."
              npm list ts-jest || echo "‚ö†Ô∏è ts-jest not found"
              
              # Install Playwright browsers for TypeScript tests
              echo "üé≠ Installing Playwright browsers for TypeScript..."
              npx playwright install --with-deps || echo "‚ö†Ô∏è Playwright install failed"
              cd ..
            fi
          fi
          
          # Setup Python environment (always needed for the test runner itself)
          python -m pip config set global.timeout 600
          python -m pip config set global.retries 5
          python -m pip install --upgrade pip
          
          # Install Python dependencies similar to unit test job
          pip install "darabonba-core>=1.0.0,<2.0.0" \
                      "alibabacloud-tea-openapi>=0.4.0rc3" \
                      "cryptography>=44.0.0" \
                      "httpx>=0.28.1" \
                      "alibabacloud-tea-util>=0.3.13" \
                      "python-dotenv>=1.0.0" \
                      "pydantic>=2.0" \
                      "playwright>=1.5.0" \
                      "loguru>=0.7.0"
          
          # Install test dependencies
          pip install pytest pytest-cov pytest-asyncio
          
          # Install AI dependencies
          pip install langchain-openai langgraph langchain-core
          
          echo "üîß Setting up environment..."
          # Ensure we're in the correct working directory
          cd /aoneci/runner/work/source || exit 1
          echo "üìÇ Current working directory: $(pwd)"
          
          # Install SDK in editable mode with correct path
          if [ -d "python" ]; then
            pip install -e ./python/
            echo "‚úÖ Python SDK installed successfully"
          else
            echo "‚ùå Python directory not found"
            ls -la
            exit 1
          fi
          
          # Set PYTHONPATH similar to unit test job
          export PYTHONPATH="${PYTHONPATH}:$(pwd)/python"
          export AGENTBAY_API_KEY="${{secrets.AGENTBAY_API_KEY}}"
          export DASHSCOPE_API_KEY="${{secrets.DASHSCOPE_API_KEY}}"
          
          echo "üöÄ Starting Smart Test Runner..."
          echo "Working directory: $(pwd)"
          echo "Python path: $PYTHONPATH"
          
          # Set up environment for test execution
          # Ensure we have the correct working directory
          cd /aoneci/runner/work/source || exit 1
          
          # Set Go environment variables if needed
          if [ "${{params.test_type}}" = "golang" ] || [ "${{params.test_type}}" = "all" ]; then
            export GOPROXY=https://goproxy.cn,direct
            export GOSUMDB=sum.golang.google.cn  
            export GOTOOLCHAIN=local
            echo "üîç Checking Go installation..."
            which go || echo "‚ùå Go not found in PATH"
            go version || echo "‚ùå Go version check failed"
          fi
          
          if [ "${{params.test_type}}" = "typescript" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "üîç Checking Node.js installation..."
            which node || echo "‚ùå Node not found in PATH"
            node --version || echo "‚ùå Node version check failed"
            which npm || echo "‚ùå npm not found in PATH"
            npm --version || echo "‚ùå npm version check failed"
          fi
          
          # Run the smart test runner
          if python scripts/smart_test_runner.py --test-type="${{params.test_type}}"; then
            echo "‚úÖ Smart Test Runner completed successfully"
            
            # Check if test report was generated
          if [ -f "test_report.md" ]; then
            echo "‚úÖ Test report generated successfully"
            
            # Display in CI Summary (Markdown format)
            echo "## ü§ñ Smart Integration Test Report" >> $AONE_CI_SUMMARY_MD
            echo "" >> $AONE_CI_SUMMARY_MD
            cat test_report.md >> $AONE_CI_SUMMARY_MD
            
            # Display test results with better formatting
            echo ""
            echo "üìã Êô∫ËÉΩÈõÜÊàêÊµãËØïËØ¶ÁªÜÁªìÊûúÔºö"
            echo ""
            
            # Show summary first
            if grep -q "## üìä ÊµãËØïÊÄªÁªì" test_report.md; then
              echo "### üìä ÊµãËØïÊÄªÁªì"
              sed -n '/## üìä ÊµãËØïÊÄªÁªì/,/^## /p' test_report.md | head -n -1 | tail -n +2
              echo ""
            fi
            
            # Show failed tests with better formatting
            if grep -q "‚ùå Â§±Ë¥•ÊµãËØï" test_report.md; then
              echo "### ‚ùå Â§±Ë¥•ÁöÑÊµãËØï"
              echo ""
              
              # Extract and display failed test IDs and basic info
              awk '
              BEGIN { 
                in_failed_test = 0;
                test_count = 0;
              }
              /‚ùå Â§±Ë¥•ÊµãËØï/ {
                in_failed_test = 1;
                test_count++;
                next;
              }
              in_failed_test && /^\*\*ÊµãËØïID\*\*:/ {
                gsub(/^\*\*ÊµãËØïID\*\*: `/, "", $0);
                gsub(/`$/, "", $0);
                print "#### " test_count ". " $0;
                next;
              }
              in_failed_test && /^\*\*ÈîôËØØ‰ø°ÊÅØ\*\*:/ {
                print $0;
                next;
              }
              in_failed_test && /^---$/ {
                in_failed_test = 0;
                print "";
                next;
              }
              ' test_report.md
              
              echo ""
              echo "::group::üîç Êü•ÁúãÂÆåÊï¥Â§±Ë¥•ÊµãËØïËØ¶ÊÉÖ"
              echo "ÂÆåÊï¥ÁöÑÂ§±Ë¥•ÊµãËØï‰ø°ÊÅØÔºàÂåÖÂê´AIÂàÜÊûêÂíå‰øÆÂ§çÂª∫ËÆÆÔºâÔºö"
              echo ""
              cat test_report.md
              echo "::endgroup::"
            else
              echo "‚úÖ ÊâÄÊúâÊµãËØïÈÉΩÈÄöËøá‰∫ÜÔºÅ"
            fi
            
            # Show successful tests summary
            if grep -q "‚úÖ ÊàêÂäüÊµãËØï" test_report.md; then
              echo ""
              echo "### ‚úÖ ÊàêÂäüÁöÑÊµãËØï"
              grep -c "‚úÖ ÊàêÂäüÊµãËØï" test_report.md | xargs -I {} echo "ÂÖ± {} ‰∏™ÊµãËØïÊàêÂäüÈÄöËøá"
            fi
            
            # Archive as build artifacts
            mkdir -p artifacts
            cp test_report.md artifacts/smart_test_report.md
          else
            echo "‚ö†Ô∏è No test report generated"
            echo "## ‚ùå Smart Integration Test Report" >> $AONE_CI_SUMMARY_MD
            echo "No test report was generated. Check the logs above for errors." >> $AONE_CI_SUMMARY_MD
          fi
          else
            echo "‚ùå Smart Test Runner failed with exit code $?"
            echo "‚ùå CI pipeline will now fail"
            exit 1
          fi

