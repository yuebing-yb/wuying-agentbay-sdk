name: "Smart Integration Test"

params:
  test_type:
    name: Test Type
    type: string
    default: "all"
    options:
      - "all"
      - "python"
      - "typescript"
      - "golang"

jobs:
  smart-integration-test:
    name: "Smart Integration Test"
    runs-on:
      - "4-16Gi"
    image: alios-8u
    timeout: 3h
    retry:
      max: 2
      when:
        - runner_system_failure
        - stuck_or_timeout_failure
    steps:
      - uses: checkout
      
      - uses: setup-env
        inputs:
          python-version: '3.12'
          node-version: '18'
          npm-version: '10'
          npm-cache: true
          go-version: '1.24.3'
          go-mod-cache: true
          go-cache: true
          
      - id: run-smart-tests
        name: "Run Smart Tests"
        run: |
          echo "üîÑ Installing dependencies for Smart Test Runner..."
          
          # Setup Golang dependencies if needed
          if [ "${{params.test_type}}" = "golang" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "üêπ Setting up Golang environment..."
            # Configure Go environment (setup-env already installed Go)
            go env -w GOPROXY=https://goproxy.cn,direct
            go env -w GOSUMDB=sum.golang.google.cn
            go env -w GOTOOLCHAIN=local
            go version
            
            if [ -d "golang" ]; then
              cd golang
              go mod download
              # Install Playwright browsers for Golang tests
              echo "üé≠ Installing Playwright browsers for Golang..."
              go run github.com/playwright-community/playwright-go/cmd/playwright@latest install --with-deps
              cd ..
            fi
          fi
          
          # Setup TypeScript dependencies if needed  
          if [ "${{params.test_type}}" = "typescript" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "üìú Setting up TypeScript environment..."
            # Node.js and npm already installed by setup-env
            node --version
            npm --version
            
            if [ -d "typescript" ]; then
              cd typescript
              # Use npm ci like the working unit tests
              npm config set fetch-timeout 300000
              npm config set fetch-retry-mintimeout 20000
              npm config set fetch-retry-maxtimeout 120000
              npm ci --prefer-offline --no-audit
              
              # Verify ts-jest installation
              echo "üîç Verifying ts-jest installation..."
              npm list ts-jest || echo "‚ö†Ô∏è ts-jest not found"
              
              # Install Playwright browsers for TypeScript tests
              echo "üé≠ Installing Playwright browsers for TypeScript..."
              npx playwright install --with-deps || echo "‚ö†Ô∏è Playwright install failed"
              cd ..
            fi
          fi
          
          # Setup Python environment (always needed for the test runner itself)
          python -m pip config set global.timeout 600
          python -m pip config set global.retries 5
          python -m pip install --upgrade pip
          
          # Install Python dependencies similar to unit test job
          pip install "darabonba-core>=1.0.0,<2.0.0" \
                      "alibabacloud-tea-openapi>=0.4.0rc3" \
                      "cryptography>=44.0.0" \
                      "httpx>=0.28.1" \
                      "alibabacloud-tea-util>=0.3.13" \
                      "python-dotenv>=1.0.0" \
                      "pydantic>=2.0" \
                      "playwright>=1.5.0" \
                      "loguru>=0.7.0"
          
          # Install test dependencies
          pip install pytest pytest-cov pytest-asyncio
          
          # Install AI dependencies
          pip install langchain-openai langgraph langchain-core
          
          echo "üîß Setting up environment..."
          # Ensure we're in the correct working directory
          cd /aoneci/runner/work/source || exit 1
          echo "üìÇ Current working directory: $(pwd)"
          
          # Install SDK in editable mode with correct path
          if [ -d "python" ]; then
            pip install -e ./python/
            echo "‚úÖ Python SDK installed successfully"
          else
            echo "‚ùå Python directory not found"
            ls -la
            exit 1
          fi
          
          # Set PYTHONPATH similar to unit test job
          export PYTHONPATH="${PYTHONPATH}:$(pwd)/python"
          export AGENTBAY_API_KEY="${{secrets.AGENTBAY_API_KEY}}"
          export DASHSCOPE_API_KEY="${{secrets.DASHSCOPE_API_KEY}}"
          
          echo "üöÄ Starting Smart Test Runner..."
          echo "Working directory: $(pwd)"
          echo "Python path: $PYTHONPATH"
          
          # Set up environment for test execution
          # Ensure we have the correct working directory
          cd /aoneci/runner/work/source || exit 1
          
          # Set Go environment variables if needed
          if [ "${{params.test_type}}" = "golang" ] || [ "${{params.test_type}}" = "all" ]; then
            export GOPROXY=https://goproxy.cn,direct
            export GOSUMDB=sum.golang.google.cn  
            export GOTOOLCHAIN=local
            echo "üîç Checking Go installation..."
            which go || echo "‚ùå Go not found in PATH"
            go version || echo "‚ùå Go version check failed"
          fi
          
          if [ "${{params.test_type}}" = "typescript" ] || [ "${{params.test_type}}" = "all" ]; then
            echo "üîç Checking Node.js installation..."
            which node || echo "‚ùå Node not found in PATH"
            node --version || echo "‚ùå Node version check failed"
            which npm || echo "‚ùå npm not found in PATH"
            npm --version || echo "‚ùå npm version check failed"
          fi
          
          # Run the smart test runner
          if python scripts/smart_test_runner.py --test-type="${{params.test_type}}"; then
            echo "‚úÖ Smart Test Runner completed successfully"
            
            # Check if test report was generated
          if [ -f "test_report.md" ]; then
            echo "‚úÖ Test report generated successfully"
            
            # Generate AI Fix Prompts for failed tests
            echo "ü§ñ Generating AI fix prompts..."
            python -c "
import re
import os

def generate_ai_fix_prompts():
    try:
        with open('test_report.md', 'r', encoding='utf-8') as f:
            report_content = f.read()
        
        # Extract failed tests and their analyses
        failed_tests = []
        lines = report_content.split('\n')
        current_test = None
        current_analysis = []
        in_analysis = False
        
        for line in lines:
            if line.startswith('### ‚ùå'):
                if current_test and current_analysis:
                    failed_tests.append({
                        'test': current_test,
                        'analysis': '\n'.join(current_analysis)
                    })
                current_test = line.replace('### ‚ùå ', '').strip()
                current_analysis = []
                in_analysis = False
            elif '<summary>ü§ñ AI Analysis</summary>' in line:
                in_analysis = True
            elif '</details>' in line and in_analysis:
                in_analysis = False
            elif in_analysis and line.strip() and not line.startswith('<'):
                current_analysis.append(line)
        
        # Add last test if exists
        if current_test and current_analysis:
            failed_tests.append({
                'test': current_test,
                'analysis': '\n'.join(current_analysis)
            })
        
        if not failed_tests:
            print('üéâ No failed tests found - no AI fix prompts needed!')
            return
        
        # Generate fix prompts file
        fix_prompts = []
        fix_prompts.append('# ü§ñ AIÂ∑•ÂÖ∑‰øÆÂ§çÊèêÁ§∫ËØç')
        fix_prompts.append('')
        fix_prompts.append('‰ª•‰∏ãÊòØÈíàÂØπÂ§±Ë¥•ÊµãËØïÁöÑAI‰øÆÂ§çÊèêÁ§∫ËØçÔºåÂèØ‰ª•Áõ¥Êé•Â§çÂà∂Á≤òË¥¥Âà∞Cursor„ÄÅClaude CodeÁ≠âAIÂ∑•ÂÖ∑‰∏≠‰ΩøÁî®Ôºö')
        fix_prompts.append('')
        fix_prompts.append('---')
        fix_prompts.append('')
        
        for i, test_info in enumerate(failed_tests, 1):
            test_name = test_info['test']
            analysis = test_info['analysis']
            
            fix_prompts.append(f'## ‰øÆÂ§çÊèêÁ§∫ËØç {i}: {test_name}')
            fix_prompts.append('')
            fix_prompts.append('```')
            fix_prompts.append('ÊàëÈúÄË¶Å‰øÆÂ§ç‰∏Ä‰∏™ÈõÜÊàêÊµãËØïÂ§±Ë¥•ÁöÑÈóÆÈ¢ò„ÄÇ‰ª•‰∏ãÊòØÊµãËØï‰ø°ÊÅØÂíåAIÂàÜÊûêÁªìÊûúÔºö')
            fix_prompts.append('')
            fix_prompts.append(f'**ÊµãËØïÂêçÁß∞**: {test_name}')
            fix_prompts.append('')
            fix_prompts.append('**AIÂàÜÊûêÁªìÊûú**:')
            fix_prompts.append(analysis)
            fix_prompts.append('')
            fix_prompts.append('ËØ∑Â∏ÆÊàëÔºö')
            fix_prompts.append('1. ÂàÜÊûêÈóÆÈ¢òÁöÑÊ†πÊú¨ÂéüÂõ†')
            fix_prompts.append('2. Êèê‰æõÂÖ∑‰ΩìÁöÑ‰øÆÂ§ç‰ª£Á†Å')
            fix_prompts.append('3. Ëß£Èáä‰øÆÂ§çÊñπÊ°àÁöÑÂéüÁêÜ')
            fix_prompts.append('4. Â¶ÇÊûúÈúÄË¶ÅÔºåÊèê‰æõÁõ∏ÂÖ≥ÁöÑÊµãËØïÈ™åËØÅÊñπÊ≥ï')
            fix_prompts.append('')
            fix_prompts.append('ËØ∑Á°Æ‰øù‰øÆÂ§çÊñπÊ°àÁ¨¶ÂêàÈ°πÁõÆÁöÑÁºñÁ†ÅËßÑËåÉÂíåÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ')
            fix_prompts.append('```')
            fix_prompts.append('')
            fix_prompts.append('---')
            fix_prompts.append('')
        
        # Save fix prompts
        with open('ai_fix_prompts.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(fix_prompts))
        
        print(f'‚úÖ Generated AI fix prompts for {len(failed_tests)} failed tests')
        
    except Exception as e:
        print(f'‚ùå Error generating AI fix prompts: {e}')

generate_ai_fix_prompts()
"
            
            # Display in CI Summary (Markdown format)
            echo "## ü§ñ Smart Integration Test Report" >> $AONE_CI_SUMMARY_MD
            echo "" >> $AONE_CI_SUMMARY_MD
            cat test_report.md >> $AONE_CI_SUMMARY_MD
            
            # Add AI fix prompts to CI summary if available
            if [ -f "ai_fix_prompts.md" ]; then
              echo "" >> $AONE_CI_SUMMARY_MD
              echo "---" >> $AONE_CI_SUMMARY_MD
              echo "" >> $AONE_CI_SUMMARY_MD
              cat ai_fix_prompts.md >> $AONE_CI_SUMMARY_MD
            fi
            
            # Also show in console for immediate viewing
            echo "üìã Test report preview:"
            head -50 test_report.md
            
            # Show AI fix prompts preview if available
            if [ -f "ai_fix_prompts.md" ]; then
              echo ""
              echo "ü§ñ AI Fix Prompts preview:"
              head -30 ai_fix_prompts.md
            fi
            
            # Archive as build artifacts
            mkdir -p artifacts
            cp test_report.md artifacts/smart_test_report.md
            if [ -f "ai_fix_prompts.md" ]; then
              cp ai_fix_prompts.md artifacts/ai_fix_prompts.md
            fi
          else
            echo "‚ö†Ô∏è No test report generated"
            echo "## ‚ùå Smart Integration Test Report" >> $AONE_CI_SUMMARY_MD
            echo "No test report was generated. Check the logs above for errors." >> $AONE_CI_SUMMARY_MD
          fi
          else
            echo "‚ùå Smart Test Runner failed with exit code $?"
            echo "‚ùå CI pipeline will now fail"
            exit 1
          fi

