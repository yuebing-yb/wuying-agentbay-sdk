import os
import subprocess
import sys
import argparse
from typing import List, Dict, Any, Optional, TypedDict
import json

# Ensure we can import standard libraries. Langchain/Langgraph availability depends on environment.
print("ğŸ” æ­£åœ¨æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–...")
print(f"Pythonå¯æ‰§è¡Œæ–‡ä»¶: {sys.executable}")
print(f"Pythonç‰ˆæœ¬: {sys.version}")
print(f"Pythonè·¯å¾„: {sys.path}")

    # Check each import individually for better error reporting
try:
    print("ğŸ“¦ æ­£åœ¨å¯¼å…¥langchain_openai...")
    from langchain_openai import ChatOpenAI
    print("âœ… langchain_openaiå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ langchain_openaiå¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ” å°è¯•æ›¿ä»£å¯¼å…¥æ–¹æ³•...")
    try:
        import langchain_openai
        print("âœ… æ›¿ä»£å¯¼å…¥æˆåŠŸ: import langchain_openai")
    except ImportError as e2:
        print(f"âŒ æ›¿ä»£å¯¼å…¥ä¹Ÿå¤±è´¥äº†: {e2}")
        
        # List available packages
        import pkgutil
        print("ğŸ“‹ åŒ…å«'langchain'çš„å¯ç”¨åŒ…:")
        for _, name, _ in pkgutil.iter_modules():
            if 'langchain' in name.lower():
                print(f"  - {name}")
        sys.exit(1)

try:
    print("ğŸ“¦ æ­£åœ¨å¯¼å…¥langgraph...")
    from langgraph.graph import StateGraph, END
    print("âœ… langgraphå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ langgraphå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

try:
    print("ğŸ“¦ æ­£åœ¨å¯¼å…¥langchain_core...")
    from langchain_core.prompts import ChatPromptTemplate
    print("âœ… langchain_coreå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ langchain_coreå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

print("âœ… æ‰€æœ‰å¿…éœ€çš„åº“éƒ½å¯¼å…¥æˆåŠŸ!")

print("ğŸ” æ­£åœ¨æ£€æŸ¥ç¯å¢ƒå˜é‡...")
agentbay_key = os.environ.get("AGENTBAY_API_KEY")
dashscope_key = os.environ.get("DASHSCOPE_API_KEY")
print(f"AGENTBAY_API_KEY: {'âœ… å·²è®¾ç½®' if agentbay_key else 'âŒ ç¼ºå¤±'}")
print(f"DASHSCOPE_API_KEY: {'âœ… å·²è®¾ç½®' if dashscope_key else 'âŒ ç¼ºå¤±'}")

# Configuration
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# Ideally we want to run this from project root, but we'll be robust
if os.path.basename(PROJECT_ROOT) == "scripts": 
    PROJECT_ROOT = os.path.dirname(PROJECT_ROOT)

TEST_DIR = os.path.join(PROJECT_ROOT, "python", "tests", "integration")
LLMS_FULL_PATH = os.path.join(PROJECT_ROOT, "llms-full.txt")
REPORT_FILE = os.path.join(PROJECT_ROOT, "test_report.md")

# State Definition
class TestResult(TypedDict):
    test_id: str
    status: str  # 'passed', 'failed', 'error'
    output: str
    error_analysis: Optional[str]

class AgentState(TypedDict):
    test_queue: List[str]
    current_test_index: int
    results: List[TestResult]
    sdk_context: str
    is_finished: bool
    specific_test_pattern: Optional[str]

# --- Helper Functions ---

def get_model():
    """Initializes the Qwen model via ChatOpenAI interface compatible with DashScope."""
    api_key = os.environ.get("DASHSCOPE_API_KEY")
    if not api_key:
        print("è­¦å‘Š: æœªæ‰¾åˆ°DASHSCOPE_API_KEYï¼Œå°†è·³è¿‡AIåˆ†æã€‚")
        return None
    
    # Using qwen-max for better reasoning capabilities on complex error logs
    return ChatOpenAI(
        model="qwen-max", 
        openai_api_key=api_key,
        openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        temperature=0.1
    )

# --- Nodes ---

def discover_tests(state: AgentState) -> AgentState:
    """Discover integration tests using pytest --collect-only."""
    print("ğŸ” æ­£åœ¨å‘ç°æµ‹è¯•...")
    pattern = state.get("specific_test_pattern")
    
    try:
        cwd = os.path.join(PROJECT_ROOT, "python")
        env = os.environ.copy()
        env["PYTHONPATH"] = cwd
        
        print(f"ğŸ“‚ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
        print(f"ğŸ“‚ å·¥ä½œç›®å½•: {cwd}")
        print(f"ğŸ“‚ PYTHONPATH: {env.get('PYTHONPATH')}")
        print(f"ğŸ” ç›®å½•å­˜åœ¨: {os.path.exists(cwd)}")
        if os.path.exists(cwd):
            print(f"ğŸ“‹ å†…å®¹: {os.listdir(cwd)}") 
        
        # Base command
        cmd = [sys.executable, "-m", "pytest", "tests/integration", "--collect-only", "-q", "-c", "/dev/null"]
        
        # Add specific test pattern if provided (passed to pytest directly for filtering)
        if pattern:
            print(f"   ä½¿ç”¨æ¨¡å¼è¿‡æ»¤æµ‹è¯•: {pattern}")
            cmd.append("-k")
            cmd.append(pattern)
            
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)} åœ¨ç›®å½• {cwd}")
        print("â³ æ­£åœ¨è¿è¡Œpytestå‘½ä»¤...")
        result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True, env=env)
        print(f"âœ… å‘½ä»¤å®Œæˆï¼Œè¿”å›ç : {result.returncode}")
        if result.stderr:
            print(f"âš ï¸ æ ‡å‡†é”™è¯¯: {result.stderr}")
        print(f"ğŸ“„ æ ‡å‡†è¾“å‡ºé•¿åº¦: {len(result.stdout)} å­—ç¬¦")
        
        test_ids = []
        for line in result.stdout.splitlines():
            line = line.strip()
            # Standard pytest -q output: tests/integration/path/to/test.py::test_name
            if line and not line.startswith("no tests ran") and not line.startswith("===") and "::" in line:
                test_id = line.split(" ")[0]
                # Fix path if it's missing tests/integration prefix
                if not test_id.startswith("tests/integration") and (test_id.startswith("_async") or test_id.startswith("_sync")):
                    test_id = os.path.join("tests", "integration", test_id)
                test_ids.append(test_id)
        
        print(f"âœ… æ‰¾åˆ° {len(test_ids)} ä¸ªæµ‹è¯•ã€‚")
        if len(test_ids) == 0 and result.stderr:
             print(f"è°ƒè¯•è¾“å‡º:\n{result.stderr}")
        
        # Load SDK Context
        context = ""
        if os.path.exists(LLMS_FULL_PATH):
            try:
                with open(LLMS_FULL_PATH, "r", encoding="utf-8") as f:
                    context = f.read()
                print(f"ğŸ“š å·²åŠ è½½SDKä¸Šä¸‹æ–‡ ({len(context)} å­—ç¬¦)")
            except Exception as e:
                print(f"âš ï¸ è¯»å–llms-full.txtå¤±è´¥: {e}")
        else:
            print(f"âš ï¸ åœ¨ {LLMS_FULL_PATH} æœªæ‰¾åˆ°llms-full.txt")

        return {
            "test_queue": test_ids,
            "current_test_index": 0,
            "results": [],
            "sdk_context": context,
            "is_finished": False,
            "specific_test_pattern": pattern
        }
    except Exception as e:
        print(f"âŒ å‘ç°æµ‹è¯•æ—¶å‡ºé”™: {e}")
        return {"test_queue": [], "current_test_index": 0, "results": [], "sdk_context": "", "is_finished": True, "specific_test_pattern": pattern}

def execute_next_test(state: AgentState) -> AgentState:
    """Executes the next test in the queue."""
    idx = state["current_test_index"]
    queue = state["test_queue"]
    
    if idx >= len(queue):
        return state 

    test_id = queue[idx]
    print(f"â–¶ï¸ æ­£åœ¨è¿è¡Œæµ‹è¯• ({idx+1}/{len(queue)}): {test_id}")
    
    cwd = os.path.join(PROJECT_ROOT, "python")
    env = os.environ.copy()
    env["PYTHONPATH"] = cwd
    
    # Ensure AGENTBAY_API_KEY is present (it should be injected by CI/Aone)
    if "AGENTBAY_API_KEY" not in env:
        print("âš ï¸ è­¦å‘Š: ç¯å¢ƒå˜é‡ä¸­æœªæ‰¾åˆ°AGENTBAY_API_KEYã€‚")

    # Run specific test
    cmd = [sys.executable, "-m", "pytest", test_id, "-vv"]
    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True, env=env)
    
    status = "passed" if result.returncode == 0 else "failed"
    output = result.stdout + "\n" + result.stderr
    
    print(f"   ç»“æœ: {status.upper()}")
    
    new_result: TestResult = {
        "test_id": test_id,
        "status": status,
        "output": output,
        "error_analysis": None
    }
    
    return {
        "results": state["results"] + [new_result],
        "current_test_index": state["current_test_index"], # Keep current index, will be incremented later
        "test_queue": state["test_queue"],
        "sdk_context": state["sdk_context"],
        "is_finished": state["is_finished"],
        "specific_test_pattern": state["specific_test_pattern"]
    }

def analyze_failure(state: AgentState) -> AgentState:
    """Analyzes the last failed test."""
    last_result = state["results"][-1]
    if last_result["status"] == "passed":
        return state 
        
    print(f"ğŸ¤– æ­£åœ¨åˆ†æå¤±è´¥æµ‹è¯• {last_result['test_id']}...")
    
    model = get_model()
    if not model:
        last_result["error_analysis"] = "è·³è¿‡åˆ†æ (æ— DASHSCOPE_API_KEY)ã€‚"
        return {"results": state["results"][:-1] + [last_result], **{k:v for k,v in state.items() if k != "results"}}

    # Prepare context
    # Limit context to avoid super long prompts if not needed, 
    # but allow enough for the model to understand the SDK.
    sdk_context_snippet = state["sdk_context"][:50000] + "...(truncated)" if len(state["sdk_context"]) > 50000 else state["sdk_context"]
    
    # Get test code
    test_file_path = os.path.join(PROJECT_ROOT, "python", last_result["test_id"].split("::")[0])
    test_code = ""
    if os.path.exists(test_file_path):
        try:
            with open(test_file_path, "r") as f:
                test_code = f.read()
        except:
            test_code = "Could not read test file."

    error_log = last_result["output"][-5000:] # Last 5000 chars of log

    prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Python SDKæµ‹è¯•ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡è¿›è¡Œåˆ†æå’Œå›ç­”ã€‚

### SDK Context (Documentation/Codebase)
{sdk_context}

### ä»»åŠ¡
åˆ†æä»¥ä¸‹é›†æˆæµ‹è¯•çš„å¤±è´¥åŸå› ã€‚
åˆ¤æ–­è¿™æ˜¯æµ‹è¯•é—®é¢˜ã€ç¯å¢ƒé—®é¢˜ï¼Œè¿˜æ˜¯SDKç¼ºé™·ã€‚

### æµ‹è¯•ä¿¡æ¯
æµ‹è¯•ID: {test_id}

æµ‹è¯•ä»£ç :
```python
{test_code}
```

é”™è¯¯æ—¥å¿—ç‰‡æ®µ:
{error_log}

### Output Instructions
è¯·ç”¨ä¸­æ–‡æä¾›ç®€æ´çš„åˆ†ææŠ¥å‘Šï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼š
1. **æ ¹æœ¬åŸå› **: å¯¼è‡´å¤±è´¥çš„å…·ä½“åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
2. **é”™è¯¯åˆ†ç±»**: [æµ‹è¯•é—®é¢˜ / ç¯å¢ƒé—®é¢˜ / SDKç¼ºé™·]
3. **ä¿®å¤å»ºè®®**: å¦‚ä½•ä¿®å¤è¿™ä¸ªé—®é¢˜ï¼ˆå¦‚é€‚ç”¨ï¼Œè¯·æä¾›ä»£ç ç‰‡æ®µï¼‰

IMPORTANT: è¯·åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸è¦ä½¿ç”¨è‹±æ–‡ã€‚
""")

    try:
        chain = prompt | model
        response = chain.invoke({
            "sdk_context": sdk_context_snippet,
            "test_id": last_result["test_id"],
            "test_code": test_code,
            "error_log": error_log
        })
        
        last_result["error_analysis"] = response.content
        print("   âœ… åˆ†æå®Œæˆã€‚")
        
    except Exception as e:
        print(f"   âŒ åˆ†æå¤±è´¥: {e}")
        last_result["error_analysis"] = f"åˆ†æå¤±è´¥: {e}"

    return {
        "results": state["results"][:-1] + [last_result],
        "test_queue": state["test_queue"],
        "current_test_index": state["current_test_index"],
        "sdk_context": state["sdk_context"],
        "is_finished": state["is_finished"],
        "specific_test_pattern": state["specific_test_pattern"]
    }

def increment_index(state: AgentState) -> AgentState:
    """Increments the test index."""
    new_index = state["current_test_index"] + 1
    print(f"ğŸ”¢ å¢åŠ ç´¢å¼•: {state['current_test_index']} -> {new_index}")
    return {
        "current_test_index": new_index,
        "results": state["results"],
        "test_queue": state["test_queue"],
        "sdk_context": state["sdk_context"],
        "is_finished": state["is_finished"],
        "specific_test_pattern": state["specific_test_pattern"]
    }

def generate_report(state: AgentState) -> AgentState:
    """Generates a Markdown report."""
    print("ğŸ“ Generating report...")
    results = state["results"]
    
    passed = len([r for r in results if r["status"] == "passed"])
    failed = len([r for r in results if r["status"] == "failed"])
    
    content = f"# Smart Integration Test Report\n\n"
    content += f"**Summary**: {len(results)} Tests | âœ… {passed} Passed | âŒ {failed} Failed\n\n"
    
    for res in results:
        icon = "âœ…" if res["status"] == "passed" else "âŒ"
        content += f"## {icon} {res['test_id']}\n\n"
        
        if res["status"] == "failed":
            content += "### ğŸ¤– AI Analysis\n"
            content += f"{res['error_analysis']}\n\n"
            
            content += "### ğŸ“„ Output (Snippet)\n"
            content += f"```\n{res['output'][-2000:]}\n```\n\n"
            
    try:
        # Save report to project root or specified artifacts dir
        report_path = os.environ.get("TEST_REPORT_PATH", REPORT_FILE)
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"Report saved to {report_path}")
    except Exception as e:
        print(f"Failed to write report: {e}")
        
    return {
        "is_finished": True,
        "results": state["results"],
        "test_queue": state["test_queue"],
        "current_test_index": state["current_test_index"],
        "sdk_context": state["sdk_context"],
        "specific_test_pattern": state["specific_test_pattern"]
    }

# --- Graph Construction ---

workflow = StateGraph(AgentState)

workflow.add_node("discover_tests", discover_tests)
workflow.add_node("execute_test", execute_next_test)
workflow.add_node("analyze_failure", analyze_failure)
workflow.add_node("increment_index", increment_index)
workflow.add_node("generate_report", generate_report)

workflow.set_entry_point("discover_tests")

def check_completion(state: AgentState):
    current_idx = state["current_test_index"]
    total_tests = len(state["test_queue"])
    print(f"ğŸ” æ£€æŸ¥å®ŒæˆçŠ¶æ€: {current_idx}/{total_tests}")
    
    if current_idx >= total_tests:
        print("âœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")
        return "generate_report"
    
    print(f"â¡ï¸ ç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯• ({current_idx + 1}/{total_tests})")
    return "execute_test"

workflow.add_conditional_edges(
    "discover_tests",
    check_completion,
    {
        "generate_report": "generate_report",
        "execute_test": "execute_test"
    }
)

def check_test_result(state: AgentState):
    last_result = state["results"][-1]
    print(f"ğŸ” æ£€æŸ¥æµ‹è¯•ç»“æœ: {last_result['test_id']} -> {last_result['status']}")
    if last_result["status"] == "failed":
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¿›è¡ŒAIåˆ†æ...")
        return "analyze_failure"
    print("âœ… æµ‹è¯•é€šè¿‡ï¼Œå¢åŠ ç´¢å¼•...")
    return "increment_index"

workflow.add_conditional_edges(
    "execute_test",
    check_test_result,
    {
        "analyze_failure": "analyze_failure",
        "increment_index": "increment_index"
    }
)

workflow.add_edge("analyze_failure", "increment_index")

workflow.add_conditional_edges(
    "increment_index",
    check_completion,
    {
        "generate_report": "generate_report",
        "execute_test": "execute_test"
    }
)

workflow.add_edge("generate_report", END)

app = workflow.compile()

def main():
    global REPORT_FILE
    
    parser = argparse.ArgumentParser(description="Smart Integration Test Runner with AI Analysis")
    parser.add_argument("-k", "--keyword", help="Run tests which match the given substring expression (same as pytest -k)", type=str)
    parser.add_argument("--test-type", help="Test type to run (all, python, typescript, golang)", type=str, default="all")
    parser.add_argument("--report", help="Path to save the report", default=REPORT_FILE)
    
    args = parser.parse_args()
    
    print("ğŸš€ Starting Smart Test Runner...")
    if args.keyword:
        print(f"ğŸ¯ Target Pattern: {args.keyword}")
    
    if args.test_type:
        print(f"ğŸ¯ Test Type: {args.test_type}")
    
    if args.report:
        REPORT_FILE = args.report

    print("ğŸ“‹ Initializing state...")
    initial_state = {
        "test_queue": [], 
        "current_test_index": 0, 
        "results": [], 
        "sdk_context": "",
        "is_finished": False,
        "specific_test_pattern": args.keyword
    }
    
    print("ğŸ”§ Starting workflow execution...")
    try:
        print("ğŸ“ About to invoke app...")
        # Set recursion limit to prevent infinite loops
        config = {"recursion_limit": 100}
        result = app.invoke(initial_state, config=config)
        print(f"âœ… Workflow completed: {result}")
    except Exception as e:
        print(f"\nğŸ’¥ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()

