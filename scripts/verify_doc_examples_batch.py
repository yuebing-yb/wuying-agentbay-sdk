#!/usr/bin/env python3
"""
æ‰¹é‡æ–‡æ¡£éªŒè¯è„šæœ¬ (Doc-Evals Pipeline)
å®ç°äº†æå–ã€é¢„æ£€ã€è¡¥å…¨ã€æ‰§è¡Œã€å½’å› ã€è‡ªæ„ˆçš„å®Œæ•´æµç¨‹ã€‚
"""

import os
import sys
import argparse
import subprocess
import shutil
import json
import hashlib
import ast
import traceback
from typing import List, Dict, Any, Optional
from collections import defaultdict
from dataclasses import dataclass, asdict

# å°è¯•å¯¼å…¥ä¾èµ–
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from markdown_it import MarkdownIt
    print("âœ… ä¾èµ–å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å®‰è£…äº†: langchain-openai, langchain-core, markdown-it-py")
    sys.exit(1)

# ================= Configuration =================

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DOCS_DIRS = [
    os.path.join(PROJECT_ROOT, "python", "docs"),
    os.path.join(PROJECT_ROOT, "docs", "guides")
]
TMP_DIR = os.path.join(PROJECT_ROOT, "tmp", "doc_verification_batch")
CACHE_FILE = os.path.join(TMP_DIR, "verification_cache.json")
LLMS_FULL_PATH = os.path.join(PROJECT_ROOT, "llms-full.txt")

os.makedirs(TMP_DIR, exist_ok=True)

# ================= Data Structures =================

@dataclass
class Snippet:
    id: str
    file_path: str
    line_number: int
    content: str
    context: str = ""
    
    @property
    def content_hash(self) -> str:
        return hashlib.sha256(self.content.encode('utf-8')).hexdigest()

@dataclass
class VerificationResult:
    file_path: str
    success: bool
    output: str
    error_type: Optional[str] = None  # DOC_FAULT, ENV_FAULT, None
    snippets_count: int = 0
    snippets: List[Snippet] = None
    
    def __post_init__(self):
        if self.snippets is None:
            self.snippets = []
    
# ================= Core Modules =================

class CacheManager:
    def __init__(self, cache_path: str):
        self.cache_path = cache_path
        self.cache = self._load_cache()
        
    def _load_cache(self) -> Dict:
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, 'r') as f:
                    return json.load(f)
            except:
                return {}
        return {}
        
    def save_cache(self):
        with open(self.cache_path, 'w') as f:
            json.dump(self.cache, f, indent=2)
            
    def get_status(self, file_path: str, content_hash: str) -> Optional[str]:
        file_cache = self.cache.get(file_path, {})
        if file_cache.get('hash') == content_hash:
            return file_cache.get('status')
        return None
        
    def update_status(self, file_path: str, content_hash: str, status: str):
        self.cache[file_path] = {
            'hash': content_hash,
            'status': status,
            'timestamp': os.path.getmtime(file_path) if os.path.exists(file_path) else 0
        }

class CodeExtractor:
    def __init__(self):
        self.md = MarkdownIt()
        
    def extract(self, file_path: str) -> List[Snippet]:
        snippets = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            tokens = self.md.parse(content)
            
            # ç®€å•çš„è¡Œå·ä¼°ç®— (markdown-it-py tokenåŒ…å«mapå±æ€§ [start_line, end_line])
            lines = content.splitlines()
            
            for token in tokens:
                if token.type == 'fence' and token.info.lower() in ['python', 'py']:
                    code = token.content
                    start_line = token.map[0] if token.map else 0
                    
                    # æå–ä¸Šä¸‹æ–‡ (å‰3è¡Œ)
                    context_start = max(0, start_line - 3)
                    context = "\n".join(lines[context_start:start_line])
                    
                    snippets.append(Snippet(
                        id=f"{os.path.basename(file_path)}:{start_line}",
                        file_path=file_path,
                        line_number=start_line + 1,
                        content=code,
                        context=context
                    ))
        except Exception as e:
            print(f"âš ï¸ è§£æå¤±è´¥ {file_path}: {e}")
            
        return snippets

    @staticmethod
    def pre_check(snippet: Snippet) -> bool:
        """AST é™æ€æ£€æŸ¥"""
        if not snippet.content.strip():
            return False
            
        # è¿‡æ»¤è§„åˆ™
        skip_patterns = [
            r'pip install',
            r'export\s+\w+=',
        ]
        import re
        for p in skip_patterns:
            if re.search(p, snippet.content, re.MULTILINE):
                return False

        try:
            ast.parse(snippet.content)
            return True
        except SyntaxError:
            # å…è®¸ä¸€äº›å¸¸è§çš„ç‰‡æ®µå¼é”™è¯¯ï¼ˆå¦‚ç¼ºå°‘importï¼‰ï¼Œä½†åœ¨ä¸¥æ ¼æ¨¡å¼ä¸‹è¿™å¯èƒ½æ˜¯ä¸€ä¸ªä¿¡å·
            # è¿™é‡Œæˆ‘ä»¬åªè¿‡æ»¤æ‰æå…¶æ˜æ˜¾çš„éä»£ç æ–‡æœ¬
            return True 
        except Exception:
            return False

class LLMGenerator:
    def __init__(self):
        api_key = os.environ.get("DASHSCOPE_API_KEY")
        if not api_key:
            raise ValueError("DASHSCOPE_API_KEY not found")
            
        self.model = ChatOpenAI(
            model="qwen-max",
            openai_api_key=api_key,
            openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
            temperature=0.1
        )
        self.sdk_context = self._load_context()
        
    def _load_context(self) -> str:
        if os.path.exists(LLMS_FULL_PATH):
            try:
                with open(LLMS_FULL_PATH, 'r') as f:
                    return f.read()[:30000] # Limit context
            except:
                pass
        return ""

    def generate_script(self, file_path: str, snippets: List[Snippet], last_error: str = "") -> str:
        snippets_text = ""
        for i, s in enumerate(snippets):
            snippets_text += f"\n--- SNIPPET {i+1} (Line {s.line_number}) ---\n{s.content}\n"
            
        prompt = """
ä½ æ˜¯ä¸€ä¸ª Python ä»£ç éªŒè¯ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†æ–‡æ¡£ä¸­çš„ Python ä»£ç ç‰‡æ®µè½¬æ¢ä¸ºå¯æ‰§è¡Œçš„æµ‹è¯•è„šæœ¬ï¼Œä»¥éªŒè¯å…¶æ­£ç¡®æ€§ã€‚

### SDK Context
{sdk_context}

### ç›®æ ‡æ–‡ä»¶: {file_path}

### å¾…éªŒè¯ä»£ç ç‰‡æ®µ:
{snippets_text}

### ä¸Šä¸€æ¬¡æŠ¥é”™ (å¦‚æœæ˜¯é‡è¯•):
{last_error}

### è¦æ±‚:
1. **å…¨é‡è¦†ç›–**: å¿…é¡»éªŒè¯ä¸Šè¿°**æ‰€æœ‰**ä»£ç ç‰‡æ®µã€‚
2. **é€»è¾‘å°è£…**: å°†æ¯ä¸ªç‰‡æ®µå°è£…ä¸ºç‹¬ç«‹çš„å‡½æ•° `def snippet_N():` (N=1,2,...)ã€‚
   - å¦‚æœç‰‡æ®µä¹‹é—´æœ‰æ˜æ˜¾çš„ä¾èµ–å…³ç³»ï¼ˆå¦‚ç‰‡æ®µ1åˆå§‹åŒ–å˜é‡ï¼Œç‰‡æ®µ2ä½¿ç”¨ï¼‰ï¼Œä½ å¯ä»¥å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€ä¸ªæµç¨‹ä¸­ï¼Œæˆ–è€…åœ¨ `snippet_N` ä¸­é‡æ–°åˆå§‹åŒ–ã€‚
   - ä¼˜å…ˆå‡è®¾ç‰‡æ®µæ˜¯ç‹¬ç«‹çš„ï¼Œé™¤éå®ƒä»¬æ˜æ˜¾å±äºåŒä¸€ä¸ªæ­¥éª¤ã€‚
3. **ç¯å¢ƒè¡¥å…¨**: 
   - è¡¥å……æ‰€æœ‰å¿…è¦çš„ `import`ã€‚
   - ä½¿ç”¨ `unittest.mock` æˆ– `tempfile` æ¨¡æ‹Ÿæ–‡ä»¶/ç½‘ç»œæ“ä½œï¼Œç¦æ­¢çœŸå®å¤–ç½‘è¯·æ±‚ã€‚
4. **é”™è¯¯å¤„ç† (å…³é”®)**:
   - **ä¸è¦** è‡ªåŠ¨ä¸ºç”¨æˆ·çš„ä»£ç æ·»åŠ  `try...except` æˆ– `pytest.raises` æ¥æ©ç›–é”™è¯¯ï¼Œé™¤éç”¨æˆ·ä»£ç æœ¬èº«åŒ…å«å¼‚å¸¸æ•è·é€»è¾‘ã€‚
   - æˆ‘ä»¬çš„ç›®æ ‡æ˜¯**å‘ç°**æ–‡æ¡£ä¸­ä¸èƒ½è¿è¡Œçš„é”™è¯¯ä»£ç ã€‚å¦‚æœç”¨æˆ·ä»£ç æ‰§è¡Œæ—¶æŠ¥é”™ï¼Œé‚£å°±æ˜¯æµ‹è¯•ä¸é€šè¿‡ã€‚
   - ä¾‹å¤–ï¼šå¦‚æœä»£ç æ³¨é‡Šä¸­æ˜ç¡®å†™äº† `# Expect Error` æˆ– `# Should raise`ï¼Œåˆ™å…è®¸ä½ æ·»åŠ æ•è·é€»è¾‘ã€‚
5. **æ‰§è¡Œå…¥å£**:
   - ç”Ÿæˆ `if __name__ == "__main__":` å—ï¼ŒæŒ‰é¡ºåºè°ƒç”¨æ‰€æœ‰ `snippet_N` å‡½æ•°ã€‚
   - ä½¿ç”¨ `try...except` åŒ…è£¹æ¯ä¸ªå‡½æ•°è°ƒç”¨ã€‚
   - å¦‚æœæ˜¯æ–‡æ¡£ä»£ç æœ¬èº«çš„é”™è¯¯ï¼ˆå¦‚APIä¸å­˜åœ¨ï¼‰ï¼Œæ‰“å° `[DOC_FAULT] snippet_N` å¹¶é€€å‡º(1)ã€‚
   - å¦‚æœæ˜¯ç¯å¢ƒ/Mockç¼ºå¤±å¯¼è‡´çš„é”™è¯¯ï¼Œæ‰“å° `[ENV_FAULT] snippet_N` å¹¶é€€å‡º(2)ã€‚
   - æ‰“å°è¯¦ç»†çš„å †æ ˆä¿¡æ¯ä»¥ä¾¿è°ƒè¯•ã€‚

### è¾“å‡ºæ ¼å¼:
åªè¿”å› Python ä»£ç ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚
"""
        chain = ChatPromptTemplate.from_template(prompt) | self.model
        resp = chain.invoke({
            "sdk_context": self.sdk_context,
            "file_path": file_path,
            "snippets_text": snippets_text,
            "last_error": last_error
        })
        
        content = resp.content.strip()
        if content.startswith("```python"):
            content = content.split("```python")[1]
        if content.endswith("```"):
            content = content[:-3]
        return content.strip()

class SandboxRunner:
    def run(self, script: str, file_path: str) -> Dict:
        # Generate unique run directory
        run_id = hashlib.md5(f"{file_path}{script}".encode()).hexdigest()[:8]
        run_dir = os.path.join(TMP_DIR, f"run_{run_id}")
        
        if os.path.exists(run_dir):
            shutil.rmtree(run_dir)
        os.makedirs(run_dir)
        
        script_path = os.path.join(run_dir, "runner.py")
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script)
            
        env = os.environ.copy()
        env["PYTHONPATH"] = os.path.join(PROJECT_ROOT, "python") + os.pathsep + env.get("PYTHONPATH", "")
        
        try:
            result = subprocess.run(
                [sys.executable, "runner.py"],
                cwd=run_dir,
                capture_output=True,
                text=True,
                env=env,
                timeout=180
            )
            
            output = result.stdout + "\n" + result.stderr
            
            # ä¼˜åŒ–è¾“å‡ºæ ¼å¼ï¼Œæ–¹ä¾¿åç»­æå–
            final_output = output
            if result.returncode != 0:
                # å°è¯•ä»è¾“å‡ºä¸­æå– Exception ä¿¡æ¯
                lines = output.strip().splitlines()
                # æŸ¥æ‰¾ç±»ä¼¼ [DOC_FAULT] çš„è¡Œ
                fault_lines = [l for l in lines if "[DOC_FAULT]" in l or "[ENV_FAULT]" in l]
                if fault_lines:
                    # å¦‚æœæœ‰æ˜ç¡®çš„FAULTæ ‡è®°ï¼Œå°è¯•æŠŠå®ƒæ”¾åœ¨æœ€åä¸€è¡Œ
                    pass
                else:
                    # å¦‚æœæ²¡æœ‰ï¼Œå¯èƒ½æ˜¯åœ¨ print ä¹‹å‰å°±å´©æºƒäº†ï¼Œä¿ç•™åŸæ ·
                    pass

            if result.returncode == 0:
                return {"status": "SUCCESS", "output": final_output}
            elif result.returncode == 1 or "DOC_FAULT" in final_output:
                return {"status": "DOC_FAULT", "output": final_output}
            else:
                return {"status": "ENV_FAULT", "output": final_output}

                
        except subprocess.TimeoutExpired:
             return {"status": "ENV_FAULT", "output": "Timeout"}
        except Exception as e:
             return {"status": "ENV_FAULT", "output": str(e)}

# ================= Main Pipeline =================

def process_file(file_path: str, 
                 extractor: CodeExtractor, 
                 generator: LLMGenerator, 
                 runner: SandboxRunner,
                 cache: CacheManager) -> VerificationResult:
    
    print(f"\nğŸ“„ Processing: {file_path}")
    
    # 1. Extraction
    snippets = extractor.extract(file_path)
    snippets = [s for s in snippets if extractor.pre_check(s)]
    
    if not snippets:
        print("   â© No executable snippets found.")
        return VerificationResult(file_path, True, "No snippets", None, 0, [])
        
    # 2. Cache Check (Combined hash of all snippets)
    combined_hash = hashlib.sha256("".join(s.content for s in snippets).encode()).hexdigest()
    cached_status = cache.get_status(file_path, combined_hash)
    
    if cached_status == "SUCCESS":
        print("   âœ… Cache hit (SUCCESS)")
        return VerificationResult(file_path, True, "Cached", None, len(snippets), snippets)
        
    # 3. Augmentation & Execution (with Retry)
    max_retries = 2
    last_error = ""
    
    for attempt in range(max_retries + 1):
        if attempt > 0:
            print(f"   ğŸ”„ Retry {attempt}/{max_retries} due to ENV_FAULT...")
            
        # Generate
        try:
            script = generator.generate_script(file_path, snippets, last_error)
        except Exception as e:
            print(f"   âŒ LLM Generation failed: {e}")
            return VerificationResult(file_path, False, str(e), "LLM_ERROR", len(snippets), snippets)
            
        # Execute
        res = runner.run(script, file_path)
        
        if res['status'] == 'SUCCESS':
            print("   âœ… Verification Passed")
            cache.update_status(file_path, combined_hash, "SUCCESS")
            cache.save_cache()
            return VerificationResult(file_path, True, res['output'], None, len(snippets), snippets)
            
        elif res['status'] == 'DOC_FAULT':
            print("   âŒ Document Fault Detected")
            return VerificationResult(file_path, False, res['output'], "DOC_FAULT", len(snippets), snippets)
            
        else: # ENV_FAULT
            last_error = res['output'][-2000:] # Capture last part of error for retry
            
    print("   âš ï¸ Environment Setup Failed after retries")
    return VerificationResult(file_path, False, last_error, "ENV_FAULT", len(snippets), snippets)

def generate_report(results: List[VerificationResult], output_path: str):
    total = len(results)
    success = len([r for r in results if r.success])
    failed = total - success
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# æ–‡æ¡£ç¤ºä¾‹ä»£ç è‡ªåŠ¨åŒ–å·¡æ£€æŠ¥å‘Š\n\n")
        
        # 1. æ¦‚è§ˆ
        f.write("## 1. æ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»è®¡æ–‡ä»¶**: {total}\n")
        f.write(f"- **âœ… é€šè¿‡**: {success}\n")
        f.write(f"- **âŒ å¤±è´¥**: {failed}\n")
        
        # 2. å¤±è´¥è¯¦æƒ…
        if failed > 0:
            f.write("\n## 2. ğŸ”´ å¤±è´¥æ–‡ä»¶è¯¦æƒ…\n\n")
            f.write("| æ–‡ä»¶è·¯å¾„ | é”™è¯¯ç±»å‹ | è¯¦æƒ… |\n")
            f.write("| :--- | :--- | :--- |\n")
            
            for r in results:
                if not r.success:
                    # å°è¯•æå–æœ€åä¸€è¡Œé”™è¯¯ä¿¡æ¯
                    error_lines = r.output.strip().splitlines()
                    last_error = error_lines[-1] if error_lines else "Unknown Error"
                    
                    # ç®€åŒ–æ–‡ä»¶è·¯å¾„
                    rel_path = os.path.relpath(r.file_path, PROJECT_ROOT)
                    
                    f.write(f"| `{rel_path}` | **{r.error_type}** | `{last_error}` |\n")
            
            f.write("\n### é”™è¯¯å †æ ˆä¸åˆ†æ\n\n")
            for r in results:
                if not r.success:
                    rel_path = os.path.relpath(r.file_path, PROJECT_ROOT)
                    f.write(f"#### ğŸ“„ {rel_path}\n")
                    f.write(f"- **é”™è¯¯ç±»å‹**: {r.error_type}\n")
                    f.write(f"- **ä»£ç ç‰‡æ®µæ•°**: {r.snippets_count}\n")
                    
                    # å°è¯•å®šä½å‡ºé”™çš„ snippet
                    import re
                    fault_match = re.search(r"\[(DOC|ENV)_FAULT\] snippet_(\d+)", r.output)
                    if fault_match:
                        snippet_idx = int(fault_match.group(2)) - 1
                        if 0 <= snippet_idx < len(r.snippets):
                            fault_snippet = r.snippets[snippet_idx]
                            f.write(f"\n**å‡ºé”™ä»£ç ç‰‡æ®µ (ç¬¬ {snippet_idx+1} æ®µ, è¡Œ {fault_snippet.line_number})**:\n")
                            f.write("```python\n")
                            f.write(fault_snippet.content)
                            f.write("\n```\n")
                    
                    # æå–å…³é”®æŠ¥é”™ä¿¡æ¯ (è¿‡æ»¤æ‰éƒ¨åˆ†æ— ç”¨çš„å †æ ˆ)
                    f.write("\n**è¿è¡Œæ—¥å¿— (éƒ¨åˆ†)**:\n")
                    f.write("```text\n")
                    # åªä¿ç•™æœ€å20è¡Œ
                    log_content = "\n".join(r.output.strip().splitlines()[-20:])
                    f.write(log_content)
                    f.write("\n```\n\n")
                    
        # 3. é€šè¿‡åˆ—è¡¨
        if success > 0:
            f.write("\n## 3. âœ… é€šè¿‡æ–‡ä»¶åˆ—è¡¨\n\n")
            for r in results:
                if r.success:
                    rel_path = os.path.relpath(r.file_path, PROJECT_ROOT)
                    f.write(f"- `{rel_path}`\n")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pattern", help="Filter files by pattern")
    parser.add_argument("--report", default="verification_report.md")
    args = parser.parse_args()
    
    # Init components
    try:
        extractor = CodeExtractor()
        generator = LLMGenerator()
        runner = SandboxRunner()
        cache = CacheManager(CACHE_FILE)
    except Exception as e:
        print(f"âŒ Initialization failed: {e}")
        sys.exit(1)
        
    results = []
    
    # Scan files
    files_to_process = []
    for doc_dir in DOCS_DIRS:
        for root, _, files in os.walk(doc_dir):
            for file in files:
                if file.endswith(".md"):
                    path = os.path.join(root, file)
                    rel_path = os.path.relpath(path, PROJECT_ROOT)
                    if args.pattern and args.pattern not in rel_path:
                        continue
                    files_to_process.append(path)
                    
    print(f"ğŸš€ Starting verification for {len(files_to_process)} files...")
    
    for file_path in files_to_process:
        res = process_file(file_path, extractor, generator, runner, cache)
        results.append(res)
        
    generate_report(results, args.report)
    print(f"\nğŸ“ Report generated: {args.report}")
    
    if any(not r.success for r in results):
        sys.exit(1)

if __name__ == "__main__":
    main()
